ADR-006 — Histórico de Conversação

Contexto

Interações com o RAG podem envolver várias perguntas sequenciais, onde o usuário faz referência a respostas anteriores. No entanto, enviar o histórico completo para o LLM é caro e pode ultrapassar o limite de tokens. É preciso um mecanismo que preserve continuidade suficiente sem comprometer custo e performance.

Decisão

Manter todo o histórico localmente, mas enviar apenas as últimas 6 mensagens para o LLM em cada requisição.

Razões

Garante continuidade de ~3 trocas completas de pergunta/resposta.

Evita estouros de contexto e reduz tokens enviados.

O histórico completo permanece disponível para logging ou auditoria.

Simples de implementar e suficiente para o domínio (consultas sobre regulamento).

Implementação

Histórico completo: armazenado em uma lista Python em memória.

Contexto enviado ao LLM: fatia das últimas 6 mensagens.

Alternativas Rejeitadas

Enviar todo o histórico: elevado custo de tokens e risco de truncamento.

Não manter histórico: compromete a continuidade e a experiência do usuário.

Consequências

Conversas mais naturais sem comprometer performance.

Controle total sobre o tamanho do prompt enviado ao modelo.