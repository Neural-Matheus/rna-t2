ADR-004 — Provedor de LLM

Contexto

O sistema de RAG precisa de um modelo generativo rápido, com boa qualidade e que permita altos volumes de requisições durante desenvolvimento e testes. Como o projeto será executado no Google Colab, não há GPU dedicada para rodar modelos localmente, tornando necessária uma API capaz de entregar boa performance sem custo adicional.

Decisão

Utilizar Groq como provedor e o Llama 3.3 70B como modelo de geração.

Razões

Gratuito para testes, permitindo iteração rápida no projeto.

Inferência extremamente rápida, ideal para loops RAG (retrieval → geração → avaliação).

Llama 70B oferece alta qualidade, especialmente em compreensão e respostas estruturadas.

Limite generoso (~14.400 req/dia), suficiente para todo o pipeline, inclusive rodadas de Fact Score.

Alternativas Rejeitadas

OpenAI / Anthropic: alta qualidade, porém pagos, inviáveis para testes extensivos.

Modelos locais: lentos ou impossíveis de rodar sem GPU dedicada.

APIs gratuitas menores: qualidade muito inferior na geração e contextualização.

Consequências

Permite um desenvolvimento rápido e barato.

Resultados de geração mais confiáveis, aumentando a consistência do RAG.